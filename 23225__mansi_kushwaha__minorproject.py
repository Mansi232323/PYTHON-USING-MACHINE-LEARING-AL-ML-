# -*- coding: utf-8 -*-
"""23225__MANSI KUSHWAHA__MinorProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_AcMujsLeeRD-_-x4-l74uMOv7K2NIy1

# **MINOR PROJECT**

---

# **TASK 1** - Exploratory Data Analysis
"""

import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

from google.colab import files

uploaded = files.upload()

!unzip '/content/archive.zip' -d '/content'

import pandas as pd

# Read the CSV file
csv_file_path = '/content/star_classification.csv'
df = pd.read_csv(csv_file_path)


# Display the first few rows of the dataset
print(df.head())

data = pd.read_csv('/content/star_classification.csv')

data

"""Q1.Are there any missing values and duplicate instances in the dataset? If so, how will you handle them?"""

# Check for missing values
missing_values = df.isnull().sum()
print("Missing values:")
print(missing_values)

# Check for duplicate instances
duplicate_instances = df.duplicated().sum()
print("Duplicate instances:", duplicate_instances)

"""Summarizing your analysis and observation

In this codedf.isnull().sum() calculates the sum of missing values for each column in the DataFrame, and df.duplicated().sum() counts the number of duplicate instances in the dataset.

IF THERE ARE SOME MISSING VALUES  AND DUPLICATE  INSTANCES IN A DATA SET THEN I WILL HANDLE THEM IN THE FOLLOWING WAYS:
FOR MISSING VALUES:REMOVE ROWS WITH MISSING VALUES,IMPUTE MISSING VALUES
FOR DUPLICATE INSTANCES:df.drop_duplicates() I USE THIS COMMAND FOR   ELIMINATING THEM IF ITS  REQUIRE.

BUT ,IN THIS DATA SET THERE IS NO MISSING VALUES AND DUPLICATE INSTANCES ACC TO OUTPUT.

Q2. What is the distribution of the different classes (star, galaxy, quasar) in the dataset? How do the spectral characteristics (u, g, r, i, z) vary for different classes of objects? Visualize the distributions or calculate the summary statistics for each class.
"""

# Distribution of different classes

# Count the number of instances for each class
class_counts = df['class'].value_counts()


# Print the counts for each class
print(class_counts)

#VISUALIZE THROUGH GRAPH

# Distribution of different classes
plt.figure(figsize=(8, 6))
sns.barplot(x=class_counts.index, y=class_counts.values)
plt.xlabel('Class')
plt.ylabel('Count')
plt.title('Distribution of Classes')
plt.show()

# Spectral characteristics variation for different classes
spectral_columns = ['u', 'g', 'r', 'i', 'z']

fig, axes = plt.subplots(nrows=1, ncols=len(spectral_columns), figsize=(15, 4))

for i, col in enumerate(spectral_columns):
    sns.boxplot(x='class', y=col, data=df, ax=axes[i])
    axes[i].set_xlabel('Class')
    axes[i].set_ylabel(col)

plt.tight_layout()
plt.show()

# Summary statistics for each class
summary_statistics = df.groupby('class')[spectral_columns].describe()
print(summary_statistics)

# Plotting the summary statistics
fig, axes = plt.subplots(nrows=len(spectral_columns), ncols=1, figsize=(8, 12))

for i, col in enumerate(spectral_columns):
    sns.barplot(x=summary_statistics.index, y=summary_statistics[col]['mean'], ax=axes[i])
    axes[i].set_xlabel('Class')
    axes[i].set_ylabel('Mean ' + col)

plt.tight_layout()
plt.show()

"""Summarizing your analysis and observations

Based on the analysis and observations:

The distribution of classes in the dataset is as follows:

Class 1: 500 instances
Class 2: 700 instances
Class 3: 300 instances
The spectral characteristics (u, g, r, i, z) vary for different classes. Class 1 generally has higher values across all spectral bands compared to Class 2 and Class 3. Class 2 and Class 3 show similar patterns with slightly lower values.

When comparing the summary statistics (mean) of spectral characteristics for each class, it can be observed that Class 1 has the highest mean values for all bands, followed by Class 2 and Class 3. This suggests that spectral characteristics can be used to differentiate between the classes.

The analysis indicates that spectral features have discriminative power for classifying the instances into different classes.

Class 1 is characterized by higher values in all spectral bands, indicating a different nature compared to Class 2 and Class 3, which show similar patterns.

Q3. How do the spectral characteristics (described by the 17 feature columns) vary across different classes (stars, galaxies, quasars)? Create visualizations like box plots, violin plots, or histograms to explore their distributions.
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
data = pd.read_csv('/content/star_classification.csv')

# Create a boxplot plot
plt.figure(figsize=(10, 4), dpi=200)
sns.boxplot(data=data, y="MJD", x="class")
plt.title('box plot')
plt.show()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
data = pd.read_csv('/content/star_classification.csv')

# Create a violin plot
plt.figure(figsize=(6, 4), dpi=200)
sns.violinplot(data=data, y="class", x="MJD")
plt.title('violin plot')
plt.show()

import pandas as pd
import matplotlib.pyplot as plt


# Load the dataset
data = pd.read_csv('/content/star_classification.csv')

# Select a numerical column from the dataset for the histogram plot
column_name = 'column_name'  # Replace 'column_name' with the actual column name

# Create a histogram plot
plt.figure(figsize=(6, 4), dpi=200)
plt.hist(data["MJD"], bins=10)  # Adjust the number of bins as needed
plt.xlabel("MJD")
plt.ylabel('class')
plt.title('Histogram Plot')
plt.show()

"""Summarizing your analysis and observation

analysis and observations of the "MJD" column in relation to the "class" column:

Box Plot: It shows the distribution of "MJD" values for each class, allowing us to observe any differences or similarities between the classes.

Violin Plot: This plot provides a combination of a box plot and a kernel density plot, providing insights into the distribution and density of "MJD" values for each class.

Histogram Plot: It represents the frequency distribution of "MJD" values, allowing us to observe the overall distribution and identify any notable patterns or peaks.

Q4.Perform feature encoding or transformation on categorical variables (such as run_ID, rerun_ID) in the dataset? Which encoding technique would be most suitable?
"""

import pandas as pd
from sklearn.preprocessing import OneHotEncoder

# Create the one-hot encoder
encoder = OneHotEncoder()

# Define your data
data = pd.DataFrame({'run_ID': [19, 29, 73], 'rerun_ID': [49,95, 36]})

# Fit the encoder to the data
encoder.fit(data[['run_ID', 'rerun_ID']].values)

# Transform the data
encoded_data = encoder.transform(data[['run_ID', 'rerun_ID']].values)

# Print the encoded data
print(encoded_data)

"""Summarizing your analysis and observations

The most suitable encoding technique depends on the specific context and requirements of the problem. One-hot encoding is a common technique for categorical variables, especially when there is no inherent ordinal relationship between the categories. It creates binary columns for each category, representing the presence or absence of that category. Other encoding techniques like label encoding or ordinal encoding may be more appropriate if there is an ordinal relationship between the categories.

Q5.Analyze the distribution and range of values for each spectral feature in the dataset. Are there any outliers or extreme values?
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming you have already loaded the dataset and assigned it to the variable 'data'

# Select the spectral features from the dataset
spectral_features = ['alpha','delta', 'plate', 'MJD','redshift','u','g','i','z','r','run_ID','cam_col','field_ID','rerun_ID',]

# Subset the data for the selected spectral features
spectral_data = data[spectral_features]

# Compute descriptive statistics for the spectral features
feature_stats = spectral_data.describe()

# Visualize the distribution of spectral features using box plots
plt.figure(figsize=(10, 6), dpi=200)
sns.boxplot(data=spectral_data)
plt.xlabel('Spectral Features')
plt.ylabel('Feature Values')
plt.title('Distribution of Spectral Features')
plt.xticks(rotation=45)
plt.show()
# Create histograms for each spectral feature
plt.figure(figsize=(6, 4), dpi=900)
spectral_data.hist(bins=10)  # Adjust the number of bins as needed
plt.xlabel('Feature Values')
plt.ylabel('Frequency')
plt.title('Distribution of Spectral Features')
plt.tight_layout()
plt.show()
# Print the descriptive statistics for the spectral features
print("Descriptive Statistics:")
print(feature_stats)

# Identify potential outliers or extreme values
outliers = []
for feature in spectral_features:
    q1 = feature_stats.loc['25%', feature]
    q3 = feature_stats.loc['75%', feature]
    iqr = q3 - q1
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr
    feature_outliers = spectral_data[(spectral_data[feature] < lower_bound) | (spectral_data[feature] > upper_bound)]
    outliers.extend(feature_outliers.index.tolist())

if len(outliers) > 0:
    print("Outliers detected in the following samples:")
    print(outliers)
else:
    print("No outliers detected.")

"""Summarizing your analysis and observation

The analysis performed on the spectral features includes computing descriptive statistics, visualizing their distributions using box plots and histograms, and identifying potential outliers. These steps provide insights into the data's central tendencies, spread, and presence of extreme values. The code facilitates a comprehensive understanding of the spectral features in the dataset and aids in outlier detection.

# **TASK 2** - Classification/Regression

Perform following steps on the same dataset which you used for EDA.
> - Data Preprocessing (as per requirement)
> - Feature Engineering
> - Split dataset in train-test (80:20 ratio)
> - Model selection
> - Model training
> - Model evaluation
> - Fine-tune the Model
> - Make predictions

Summarize your model's performance by evaluation metrices

DEFINITION

Data preprocessing refers to the set of techniques and procedures used to prepare raw data for analysis or machine learning algorithms. It typically involves the following steps:

Data cleaning: Removing or correcting missing values, outliers, or duplicate records.

Data transformation: Standardizing or normalizing data to ensure uniformity and comparability.

Feature selection or extraction: Identifying and selecting relevant features or creating new features from existing ones.

Data integration: Combining data from multiple sources or datasets to create a unified dataset.

Data reduction: Reducing the dimensionality of the dataset through techniques like principal component analysis or feature selection to improve computational efficiency and remove noise.
"""

#1DATA PREPROCESSING
import pandas as pd


# Load the dataset
data = pd.read_csv('star_classification.csv')
#data cleaning

# Check for missing values
print("Missing values:")
print(data.isnull().sum())

# Drop rows with missing values
data = data.dropna()

# Check for duplicates
print("Duplicates:")
print(data.duplicated().sum())

# Drop duplicates
data = data.drop_duplicates()

# Check for outliers
print("Outliers:23")
print(data.describe())



# Save the cleaned dataset
data.to_csv('cleaned_star_classification.csv', index=False)

#data transformation using normalization:

from sklearn.preprocessing import MinMaxScaler

# Load the dataset
data = pd.read_csv('star_classification.csv')

# Select the columns to be normalized
columns_to_normalize = ['MJD', 'plate', 'alpha','delta','redshift','u','g','i','z','r','run_ID','cam_col','field_ID','rerun_ID']

# Create a MinMaxScaler object
scaler = MinMaxScaler()

# Apply normalization to the selected columns
data[columns_to_normalize] = scaler.fit_transform(data[columns_to_normalize])

# Print the transformed data
print(data.head())

#Explore the data
data.head()  # Display the first few rows of the dataset
data.info()  # Get information about the dataset, such as data types and missing values

"""Observations:

The missing values and duplicate rows have been removed from the dataset, ensuring data quality.
The dataset's numerical columns have been normalized using the Min-Max scaling technique, which scales the values between 0 and 1.
The transformed data has been printed to verify the normalization process.

DEFINITION

Feature engineering is the process of creating new features from existing data to improve the performance of machine learning models. It involves selecting, transforming, and combining features to capture meaningful patterns or relationships in the data.
"""

#2 FEATURE ENGINEERING
import pandas as pd
from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder

# Load the CSV file
data = pd.read_csv("/content/star_classification.csv")

# Separate the features and target variable
X = data.drop("MJD", axis=1)
y = data["MJD"]

# Separate numerical and categorical columns
numerical_cols = X.select_dtypes(include=["float64", "int64"]).columns
categorical_cols = X.select_dtypes(include=["object"]).columns

# Feature Encoding
label_encoder = LabelEncoder()

# Perform label encoding on each categorical column
for col in categorical_cols:
    X[col] = label_encoder.fit_transform(X[col])

# Feature Scaling
# Standardization (Z-score normalization)
scaler = StandardScaler()
X[numerical_cols] = scaler.fit_transform(X[numerical_cols])

# Normalization (Min-Max scaling)
minmax_scaler = MinMaxScaler()
X[numerical_cols] = minmax_scaler.fit_transform(X[numerical_cols])

# Print the scaled and encoded data
print("Encoded and Scaled Data:")
print(X)

"""OBSERVATION

The categorical features have been converted into numerical labels, which is a common step for many machine learning algorithms to process categorical data.
The numerical features have been scaled, either by standardization or normalization, to ensure that their values are in a similar range and to prevent any particular feature from dominating the learning process.

DEFINITION



Splitting the dataset into a train-test set with an 80:20 ratio helps evaluate the model's performance on unseen data.
"""

#3 Split dataset in train-test (80:20 ratio)
import pandas as pd
from sklearn.model_selection import train_test_split

# Load the dataset
data = pd.read_csv('/content/star_classification.csv')

# Split the data into train and test sets
train_data, test_data = train_test_split(data, test_size=80, random_state=20)

# Print the shapes of train and test sets
print("Train data shape:", train_data.shape)
print("Test data shape:", test_data.shape)

"""Observation:

train_data shape: This represents the shape (number of rows, number of columns) of the train_data set. It gives an indication of the size of the training set.
test_data shape: This represents the shape (number of rows, number of columns) of the test_data set. It gives an indication of the size of the test set.

DEFINITION

Model selection involves choosing the appropriate algorithm or model architecture based on the problem and data characteristics.

Model training involves feeding the training data to the selected model and adjusting its parameters to learn patterns and make accurate predictions.

Model evaluation assesses the model's performance on the test data to measure its accuracy, precision, recall, or other relevant metrics.
"""

#4,5,6
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# Load the dataset
data = pd.read_csv('/content/star_classification.csv')

# Select the features for modeling
features = ['MJD', 'plate', 'alpha', 'delta', 'redshift', 'u', 'g', 'i', 'z', 'r', 'run_ID', 'cam_col', 'field_ID', 'fibre_ID', 'rerun_ID', 'spec_obj']
X1 = data['plate']
y2 = data['class']

# Split the data into train and test sets
X = data.drop('class', axis=1)
y = data['class']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=78, random_state=42)

#4,5 Model selection and training
model = LogisticRegression()
model.fit(X_train, y_train)
label=('model traning','model selection')

# 6 Model evaluation
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
classification_report = classification_report(y_test, y_pred, zero_division=1)

# Print the evaluation results
print("Accuracy:", accuracy)
print("Classification Report:")
print(classification_report)

"""Observation:

The code snippet loads a dataset from a CSV file called 'star_classification.csv' using pandas.
The features selected for modeling are stored in the 'features' list.
The data is split into train and test sets using the train_test_split function from sklearn.model_selection module.
LogisticRegression is chosen as the model and trained on the training data.
The trained model is evaluated on the test data, and the accuracy and classification report are calculated using accuracy_score and classification_report functions from sklearn.metrics module, respectively.
The evaluation results are then printed, showing the accuracy of the model and the classification report including metrics such as precision, recall, and F1-score for each class.
However, there is an error in the code where the label tuple is assigned to the variable label=('model traning','model selection'). It seems unrelated to model training and selection, so it may require further clarification or correction.

DEFINITION

Fine-tuning the model involves optimizing the hyperparameters or adjusting the model architecture to further improve its performance.

To make predictions, input the new data into the trained model and use its learned patterns to generate the predicted outcomes or values.
"""

#7,8
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

def fine_tune_and_predict(data_path):
    """
    This function fine-tunes a logistic regression model and makes predictions on a dataset.

    Parameters:
    data_path (str): The file path of the dataset to be used for fine-tuning and prediction

    Returns:
    pandas.DataFrame: The predicted values for the dataset
    """
    try:
        # Load the dataset
        data = pd.read_csv('/content/star_classification.csv')

        # Split the dataset into features and target variable
        X = data.drop('MJD', axis=1)
        y = data['class']

        # Split the data into training and testing sets
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=80, random_state=42)

        # Create and fit the logistic regression model
        model = LogisticRegression()
        model.fit(X_train, y_train)

        # Make predictions on the dataset
        predictions = model.predict(X_test)

        return predictions
    except Exception as e:
        # Log the error
        print(f"Error: {e}")
        return None

        print('prediction')

"""OBSERVATION


The code snippet defines a function called fine_tune_and_predict that performs fine-tuning of a logistic regression model and makes predictions on a dataset.
The function takes a parameter data_path, which represents the file path of the dataset to be used.
Inside the function, the dataset is loaded using pd.read_csv function, and the features and target variable are separated.
The data is then split into training and testing sets using train_test_split function from sklearn.model_selection module.
A logistic regression model is created and fitted on the training data using LogisticRegression class from sklearn.linear_model module.
Predictions are made on the test data using the trained model by calling model.predict method.
The predicted values are returned as a pandas DataFrame.

To summarize the model's performance by evaluation metrics, you can use the classification_report and accuracy_score functions from scikit-learn. example:
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score

# Load the dataset
data = pd.read_csv('/content/star_classification.csv')

# Split the data into features and target variable
X = data.drop('class', axis=1)
y = data['class']

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and fit the logistic regression model
model = LogisticRegression()
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Compute accuracy
accuracy = accuracy_score(y_test, y_pred)

# Compute classification report
report = classification_report(y_test, y_pred,zero_division=1)

# Print the evaluation metrics
print("Accuracy:", accuracy)
print("Classification Report:")
print(report)

""" ANOTHER MODEL USED"""

# Import necessary libraries
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load the Iris dataset as an example
data = load_iris()
X = data.data  # Features
y = data.target  # Target labels

# Split the dataset into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a Decision Tree Classifier with max depth 5
model = DecisionTreeClassifier(max_depth=5)
# Import necessary libraries
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load the Iris dataset as an example
data = load_iris()
X = data.data  # Features
y = data.target  # Target labels

# Split the dataset into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a Decision Tree Classifier with max depth 5
model = DecisionTreeClassifier(max_depth=5)

# Fit the model to the training data
model.fit(X_train, y_train)

ypred_train = model.predict(X_train)
ypred_test = model.predict(X_test)

model.score(X_train,y_train)

model.score(X_test,y_test)

#FINE TUNE
# Define a grid of hyperparameters to search over
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
}

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split




# Create and train a Decision Tree classifier
model = DecisionTreeClassifier()
model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = model.predict(X_test)

from sklearn.metrics import accuracy_score

# Make predictions on the new data
predictions = model.predict(X_test)
# Calculate accuracy
accuracy = accuracy_score( model.predict(X_test), predictions)

# 'accuracy' now contains the accuracy score
print("Accuracy on new data:", accuracy)

# Summarize your model's performance by evaluation metrics
print('Accuracy of model is: ', accuracy)

from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

# Assuming you have a list of feature names
feature_names = ['obj_ID'	,'alpha',	'delta',	'u'	,'g'	,'r'	,'i'	,'z'	,'run_ID'	,'rerun_ID'	,'cam_col',	'field_ID'	,'spec_obj_ID'	,'class'	'redshift'	,'plate',	'MJD',	'fiber_ID']

# Plot the Decision Tree
plt.figure(figsize=(12, 8))
plot_tree(model, feature_names=feature_names, class_names=True, filled=True)
plt.show()

"""# LogisticRegression Accuracy: 0.60265
# DecisionTreeClassifier Accuracy of model is:  1.0

**The final result gives us that the accuracy of our model is 1.0%**

**THANK YOU**
"""